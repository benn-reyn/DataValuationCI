import requests
import re
import pandas as pd
import time
from concurrent.futures import ThreadPoolExecutor

API_KEY = "4c700d399bfe02685f6d4b2fa57e540046c3d03b" 
SEARCH_KEYWORDS = "automotive data breach"
OUTPUT_CSV = "lawsuit_data_leakages.csv"
RESULTS_PER_PAGE = 50 #this is max
MAX_PAGES = 5  
SLEEP_BETWEEN_REQUESTS = 1  #per robots.txt
MAX_WORKERS = 5


HEADERS = {
    "Authorization": f"Token {API_KEY}"
}


def search_courtlistener_v4(query, page_url=None):
    if page_url:
        url = page_url
        params = None
    else:
        url = "https://www.courtlistener.com/api/rest/v4/opinions/"
        params = {
            "search": query,
            "page_size": RESULTS_PER_PAGE
        }
    print(f"Querying: {url}")
    response = requests.get(url, params=params, headers=HEADERS)
    if response.status_code == 200:
        return response.json()
    else:
        print(f"Error: {response.status_code} â€” {response.text}")
        return None

def download_case_text(resource_uri):
    resp = requests.get(resource_uri, headers=HEADERS)
    if resp.status_code == 200:
        data = resp.json()
        return data.get("plain_text", "")
    return ""

def fetch_cluster_name(cluster_url):
    if not cluster_url:
        return "Unknown"
    resp = requests.get(cluster_url, headers=HEADERS)
    if resp.status_code == 200:
        cluster_data = resp.json()
        return cluster_data.get("caseName", "Unknown")
    return "Unknown"

def extract_penalty_and_reason(text):
    penalties = []
    reasons = []

    # Regex for monetary amounts (e.g., $1,000,000.00)
    amount_pattern = r"\$\s?\d{1,3}(?:,\d{3})*(?:\.\d{2})?"
    amounts = re.findall(amount_pattern, text)

    if amounts:
        # Clean amounts to floats
        amounts_clean = []
        for amt in amounts:
            cleaned = amt.replace("$", "").replace(",", "").strip()
            try:
                amounts_clean.append(float(cleaned))
            except ValueError:
                continue

        if amounts_clean:
            total_amount = sum(amounts_clean)
            penalties.append("${:,.2f}".format(total_amount))  # nicely formatted

        lowered_text = text.lower()
        if "consumer" in lowered_text:
            reasons.append("consumer data")
        if "production" in lowered_text or "manufacturing" in lowered_text:
            reasons.append("production data")
        if "automotive" in lowered_text:
            reasons.append("automotive manufacturing")
        if not reasons:
            reasons.append("unspecified")

    return penalties, reasons


results = []
page_url = None

while True:
    data = search_courtlistener_v4(SEARCH_KEYWORDS, page_url)

    if not data or "results" not in data:
        print("No results found or end of pages.")
        break

    if not isinstance(data["results"], list):
        print("Unexpected results type:", type(data["results"]))
        break

    def process_case(case):
        if not isinstance(case, dict):
            return None

        cluster_url = case.get("cluster")
        case_name = fetch_cluster_name(cluster_url) if isinstance(cluster_url, str) else "Unknown"

        resource_uri = case.get("resource_uri", "")
        court_date = case.get("dateFiled") or case.get("date_filed") or ""
        case_url = case.get("absolute_url", "")

        if not resource_uri:
            return None

        text = download_case_text(resource_uri)
        if not text:
            return None

        penalties, reasons = extract_penalty_and_reason(text)

        # AUTOMOTIVE MANUFACTURING FILTER
        if not any(reason in reasons for reason in ["automotive manufacturing", "production data"]):
            return None  # Skip irrelevant cases

        return {
            "case_name": case_name,
            "case_url": "https://www.courtlistener.com" + case_url,
            "date": court_date,
            "penalty_amounts": ", ".join(penalties),
            "penalty_reason": ", ".join(reasons)
        }

    # Multithread processing
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        processed_cases = list(executor.map(process_case, data["results"]))

    results.extend([case for case in processed_cases if case])

    # Pagination
    page_url = None
    if isinstance(data.get("next"), str):
        page_url = data["next"]
    if not page_url:
        break

    time.sleep(SLEEP_BETWEEN_REQUESTS)

if results:
    df = pd.DataFrame(results)
    df.to_csv(OUTPUT_CSV, index=False)
    print(f"Scraping complete. Results saved to {OUTPUT_CSV}")
else:
    print("No relevant automotive manufacturing cases found.")


