{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0fb2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 10:26:03,391 - INFO - Starting SEC cybersecurity research session: 1d5de285\n",
      "2025-10-30 10:26:03,395 - INFO - Output file: sec_cybersecurity_research_20251030_102603.csv\n",
      "2025-10-30 10:26:03,402 - INFO - Processing Apple Inc (CIK: 0000320193)\n",
      "2025-10-30 10:26:04,119 - INFO - Request 1 to data.sec.gov (attempt 1)\n",
      "2025-10-30 10:26:04,651 - INFO - Found 5 recent 8-K filings for Apple Inc\n",
      "2025-10-30 10:26:06,003 - INFO - Request 2 to www.sec.gov (attempt 1)\n",
      "2025-10-30 10:26:06,360 - WARNING - Request blocked for www.sec.gov. Cooling off for 289.9s\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "import logging\n",
    "from urllib.parse import urlparse\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, List, Dict, Any\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class RequestConfig:\n",
    "    \"\"\"Configuration for request behavior and rate limiting\"\"\"\n",
    "    min_delay: float = 1.0\n",
    "    max_delay: float = 3.0\n",
    "    max_retries: int = 3\n",
    "    timeout: int = 30\n",
    "    backoff_base: float = 2.0\n",
    "    jitter: float = 0.3\n",
    "\n",
    "class AdvancedDataCollector:\n",
    "    \"\"\"\n",
    "    State-of-the-art data collection with anti-detection features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: RequestConfig = None):\n",
    "        self.config = config or RequestConfig()\n",
    "        self.session = requests.Session()\n",
    "        self.domain_trackers = {}\n",
    "        self.request_count = 0\n",
    "        self.session_id = hashlib.md5(str(time.time()).encode()).hexdigest()[:8]\n",
    "        \n",
    "        # Realistic browser headers rotation pool\n",
    "        self.user_agents = [\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0',\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "        ]\n",
    "        \n",
    "        self._rotate_headers()\n",
    "        \n",
    "    def _rotate_headers(self):\n",
    "        \"\"\"Rotate session headers to mimic different browser instances\"\"\"\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': random.choice(self.user_agents),\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'none',\n",
    "            'Cache-Control': 'max-age=0'\n",
    "        })\n",
    "    \n",
    "    def _get_domain_tracker(self, domain: str) -> Dict:\n",
    "        \"\"\"Get or create domain-specific request tracker\"\"\"\n",
    "        if domain not in self.domain_trackers:\n",
    "            self.domain_trackers[domain] = {\n",
    "                'last_request': 0,\n",
    "                'consecutive_errors': 0,\n",
    "                'total_requests': 0,\n",
    "                'backoff_multiplier': 1.0\n",
    "            }\n",
    "        return self.domain_trackers[domain]\n",
    "    \n",
    "    def _calculate_intelligent_delay(self, domain: str) -> float:\n",
    "        \"\"\"Calculate dynamic delay based on domain history\"\"\"\n",
    "        tracker = self._get_domain_tracker(domain)\n",
    "        \n",
    "        if 'sec.gov' in domain:\n",
    "            base_delay = random.uniform(0.8, 1.2)\n",
    "        else:\n",
    "            base_delay = random.uniform(self.config.min_delay, self.config.max_delay)\n",
    "        \n",
    "        base_delay *= tracker['backoff_multiplier']\n",
    "        jitter = random.uniform(-self.config.jitter, self.config.jitter)\n",
    "        final_delay = max(0.1, base_delay + jitter)\n",
    "        \n",
    "        time_since_last = time.time() - tracker['last_request']\n",
    "        if time_since_last < final_delay:\n",
    "            final_delay = time_since_last + random.uniform(0.1, 0.5)\n",
    "        \n",
    "        tracker['last_request'] = time.time()\n",
    "        return final_delay\n",
    "    \n",
    "    def _handle_rate_limit(self, domain: str, attempt: int):\n",
    "        \"\"\"Handle rate limiting with exponential backoff\"\"\"\n",
    "        tracker = self._get_domain_tracker(domain)\n",
    "        tracker['consecutive_errors'] += 1\n",
    "        tracker['backoff_multiplier'] *= self.config.backoff_base\n",
    "        \n",
    "        backoff_time = (self.config.backoff_base ** attempt) + random.uniform(1, 5)\n",
    "        logger.warning(f\"Rate limit hit for {domain}. Backing off for {backoff_time:.1f}s\")\n",
    "        time.sleep(backoff_time)\n",
    "        self._rotate_headers()\n",
    "    \n",
    "    def _handle_blocked_request(self, domain: str, attempt: int):\n",
    "        \"\"\"Handle blocked requests with progressive countermeasures\"\"\"\n",
    "        tracker = self._get_domain_tracker(domain)\n",
    "        tracker['consecutive_errors'] += 1\n",
    "        \n",
    "        if attempt == 1:\n",
    "            wait_time = random.uniform(10, 20)\n",
    "            self._rotate_headers()\n",
    "        elif attempt == 2:\n",
    "            wait_time = random.uniform(30, 60)\n",
    "            self.session = requests.Session()\n",
    "            self._rotate_headers()\n",
    "        else:\n",
    "            wait_time = random.uniform(120, 300)\n",
    "        \n",
    "        logger.warning(f\"Request blocked for {domain}. Cooling off for {wait_time:.1f}s\")\n",
    "        time.sleep(wait_time)\n",
    "    \n",
    "    def make_request(self, url: str, custom_headers: Optional[Dict] = None) -> Optional[requests.Response]:\n",
    "        \"\"\"Make a resilient HTTP request with comprehensive anti-detection features\"\"\"\n",
    "        domain = urlparse(url).netloc\n",
    "        tracker = self._get_domain_tracker(domain)\n",
    "        \n",
    "        for attempt in range(self.config.max_retries):\n",
    "            try:\n",
    "                delay = self._calculate_intelligent_delay(domain)\n",
    "                if delay > 0:\n",
    "                    time.sleep(delay)\n",
    "                \n",
    "                headers = self.session.headers.copy()\n",
    "                if custom_headers:\n",
    "                    headers.update(custom_headers)\n",
    "                \n",
    "                logger.info(f\"Request {self.request_count + 1} to {domain} (attempt {attempt + 1})\")\n",
    "                response = self.session.get(\n",
    "                    url,\n",
    "                    headers=headers,\n",
    "                    timeout=self.config.timeout,\n",
    "                    allow_redirects=True\n",
    "                )\n",
    "                \n",
    "                self.request_count += 1\n",
    "                tracker['total_requests'] += 1\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    tracker['consecutive_errors'] = 0\n",
    "                    tracker['backoff_multiplier'] = max(1.0, tracker['backoff_multiplier'] * 0.9)\n",
    "                    return response\n",
    "                    \n",
    "                elif response.status_code == 429:\n",
    "                    self._handle_rate_limit(domain, attempt)\n",
    "                    continue\n",
    "                    \n",
    "                elif response.status_code in [403, 503]:\n",
    "                    self._handle_blocked_request(domain, attempt)\n",
    "                    continue\n",
    "                    \n",
    "                elif 500 <= response.status_code < 600:\n",
    "                    logger.warning(f\"Server error {response.status_code} for {url}\")\n",
    "                    time.sleep((attempt + 1) * 5)\n",
    "                    continue\n",
    "                    \n",
    "                else:\n",
    "                    logger.warning(f\"HTTP {response.status_code} for {url}\")\n",
    "                    if attempt < self.config.max_retries - 1:\n",
    "                        time.sleep((attempt + 1) * 3)\n",
    "                        continue\n",
    "                    else:\n",
    "                        return None\n",
    "                        \n",
    "            except requests.exceptions.Timeout:\n",
    "                logger.warning(f\"Timeout on attempt {attempt + 1} for {url}\")\n",
    "                if attempt < self.config.max_retries - 1:\n",
    "                    time.sleep((attempt + 1) * 5)\n",
    "                    continue\n",
    "                    \n",
    "            except requests.exceptions.ConnectionError:\n",
    "                logger.warning(f\"Connection error on attempt {attempt + 1} for {url}\")\n",
    "                if attempt < self.config.max_retries - 1:\n",
    "                    time.sleep((attempt + 1) * 10)\n",
    "                    self.session = requests.Session()\n",
    "                    self._rotate_headers()\n",
    "                    continue\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"Request exception on attempt {attempt + 1}: {e}\")\n",
    "                if attempt < self.config.max_retries - 1:\n",
    "                    time.sleep((attempt + 1) * 8)\n",
    "                    continue\n",
    "        \n",
    "        logger.error(f\"All {self.config.max_retries} attempts failed for {url}\")\n",
    "        return None\n",
    "\n",
    "class SECDataProcessor:\n",
    "    \"\"\"Process SEC filing data for cybersecurity and data breach information\"\"\"\n",
    "    \n",
    "    # Comprehensive breach-related patterns\n",
    "    BREACH_PATTERNS = [\n",
    "        # Data breach mentions\n",
    "        r'(data breach|cybersecurity incident|security incident|unauthorized access|data security)',\n",
    "        # Financial impact patterns\n",
    "        r'(\\$[\\d,]+(?:\\.\\d{2})?)\\s*(?:million|billion|thousand)?\\s*(?:fine|penalty|settlement|charge|loss)',\n",
    "        # Regulatory mentions\n",
    "        r'(FTC|SEC|GDPR|CCPA|regulatory|compliance)\\s*(?:action|fine|investigation)',\n",
    "        # Incident descriptions\n",
    "        r'(compromised|exposed|leaked|breached)\\s*(?:data|information|records)',\n",
    "        # Customer impact\n",
    "        r'(customer data|personal information|PII|records)\\s*(?:exposed|compromised)'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.compiled_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.BREACH_PATTERNS]\n",
    "    \n",
    "    def extract_breach_mentions(self, text: str, company: str, filing_date: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract potential breach-related information from filing text\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        findings = []\n",
    "        text_clean = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        for i, pattern in enumerate(self.compiled_patterns):\n",
    "            matches = pattern.finditer(text_clean)\n",
    "            for match in matches:\n",
    "                # Get context around the match\n",
    "                start = max(0, match.start() - 200)\n",
    "                end = min(len(text_clean), match.end() + 200)\n",
    "                context = text_clean[start:end]\n",
    "                \n",
    "                finding = {\n",
    "                    'company': company,\n",
    "                    'filing_date': filing_date,\n",
    "                    'match_type': self._get_match_type(i),\n",
    "                    'matched_text': match.group(0),\n",
    "                    'context': context,\n",
    "                    'relevance_score': self._calculate_relevance(match.group(0), context),\n",
    "                    'extraction_timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                findings.append(finding)\n",
    "        \n",
    "        return findings\n",
    "    \n",
    "    def _get_match_type(self, pattern_index: int) -> str:\n",
    "        \"\"\"Categorize the type of breach mention\"\"\"\n",
    "        types = [\n",
    "            'breach_incident',\n",
    "            'financial_impact', \n",
    "            'regulatory_action',\n",
    "            'incident_description',\n",
    "            'customer_impact'\n",
    "        ]\n",
    "        return types[pattern_index] if pattern_index < len(types) else 'general_mention'\n",
    "    \n",
    "    def _calculate_relevance(self, matched_text: str, context: str) -> float:\n",
    "        \"\"\"Calculate relevance score for the finding\"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # Boost score for financial amounts\n",
    "        if '$' in matched_text:\n",
    "            score += 0.3\n",
    "        \n",
    "        # Boost for specific breach terminology\n",
    "        breach_terms = ['breach', 'compromised', 'unauthorized', 'exposed']\n",
    "        if any(term in matched_text.lower() for term in breach_terms):\n",
    "            score += 0.4\n",
    "        \n",
    "        # Context-based scoring\n",
    "        if 'customer' in context.lower() or 'data' in context.lower():\n",
    "            score += 0.2\n",
    "        \n",
    "        # Penalize very short matches\n",
    "        if len(matched_text.strip()) < 10:\n",
    "            score -= 0.1\n",
    "        \n",
    "        return min(1.0, max(0.1, score))\n",
    "\n",
    "class SECDataCollector:\n",
    "    \"\"\"Main class for collecting and processing SEC data\"\"\"\n",
    "    \n",
    "    # Comprehensive list of tech and automotive tech companies with CIKs\n",
    "    COMPANY_DATABASE = {\n",
    "        # Technology Companies\n",
    "        'Apple Inc': '0000320193',\n",
    "        'Microsoft Corp': '0000789019',\n",
    "        'Google (Alphabet Inc)': '0001652044',\n",
    "        'Amazon.com Inc': '0001018724',\n",
    "        'Meta Platforms Inc': '0001326801',\n",
    "        'Tesla Inc': '0001318605',\n",
    "        'NVIDIA Corp': '0001045810',\n",
    "        'Intel Corp': '0000050863',\n",
    "        'Adobe Inc': '0000796343',\n",
    "        'Salesforce Inc': '0001108524',\n",
    "        'Cisco Systems Inc': '0000858877',\n",
    "        'Oracle Corp': '0001341439',\n",
    "        'IBM Corp': '0000051143',\n",
    "        'Qualcomm Inc': '0000804328',\n",
    "        'Broadcom Inc': '0001060492',\n",
    "        \n",
    "        # Automotive Technology Companies\n",
    "        'Ford Motor Co': '0000037996',\n",
    "        'General Motors Co': '0001467858',\n",
    "        'Toyota Motor Corp': '0001094517',\n",
    "        'Honda Motor Co Ltd': '0001014156',\n",
    "        'BMW AG': '0001396269',\n",
    "        'Mercedes-Benz Group AG': '0001562634',\n",
    "        'Volkswagen AG': '0001446864',\n",
    "        'Rivian Automotive Inc': '0001874178',\n",
    "        'Lucid Group Inc': '0001428762',\n",
    "        'NIO Inc': '0001736544',\n",
    "        'XPeng Inc': '0001789020',\n",
    "        'Li Auto Inc': '0001791706',\n",
    "        \n",
    "        # Automotive Tech Suppliers\n",
    "        'Aptiv PLC': '0001521332',\n",
    "        'Magna International Inc': '0001236572',\n",
    "        'Continental AG': '0001444686',\n",
    "        'DENSO Corp': '0001446597',\n",
    "        'Mobileye Global Inc': '0001910138'\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.config = RequestConfig(min_delay=0.8, max_delay=1.5, max_retries=3)\n",
    "        self.collector = AdvancedDataCollector(self.config)\n",
    "        self.processor = SECDataProcessor()\n",
    "        self.csv_filename = f\"sec_cybersecurity_research_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        \n",
    "    def get_sec_headers(self) -> Dict[str, str]:\n",
    "        \"\"\"Get SEC-compliant headers\"\"\"\n",
    "        return {\n",
    "            'User-Agent': 'Research Organization research@organization.com',\n",
    "            'Accept': 'application/json'\n",
    "        }\n",
    "    \n",
    "    def get_company_filings(self, cik: str) -> Optional[Dict]:\n",
    "        \"\"\"Get recent filings for a company\"\"\"\n",
    "        url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "        response = self.collector.make_request(url, self.get_sec_headers())\n",
    "        \n",
    "        if response and response.status_code == 200:\n",
    "            return response.json()\n",
    "        return None\n",
    "    \n",
    "    def get_filing_content(self, accession_number: str, cik: str) -> Optional[str]:\n",
    "        \"\"\"Get the full text content of a specific filing\"\"\"\n",
    "        # Convert accession number to filing URL\n",
    "        acc_no_clean = accession_number.replace('-', '')\n",
    "        url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{acc_no_clean}/{accession_number}.txt\"\n",
    "        \n",
    "        response = self.collector.make_request(url)\n",
    "        if response and response.status_code == 200:\n",
    "            return response.text\n",
    "        return None\n",
    "    \n",
    "    def extract_recent_filings(self, filings_data: Dict, filing_type: str = '8-K') -> List[Dict]:\n",
    "        \"\"\"Extract recent filings of specific type\"\"\"\n",
    "        recent_filings = []\n",
    "        \n",
    "        try:\n",
    "            filings = filings_data.get('filings', {}).get('recent', {})\n",
    "            accession_numbers = filings.get('accessionNumber', [])\n",
    "            filing_dates = filings.get('filingDate', [])\n",
    "            forms = filings.get('form', [])\n",
    "            primary_documents = filings.get('primaryDocument', [])\n",
    "            \n",
    "            for i, form in enumerate(forms):\n",
    "                if form == filing_type and i < len(accession_numbers):\n",
    "                    filing_info = {\n",
    "                        'accession_number': accession_numbers[i],\n",
    "                        'filing_date': filing_dates[i],\n",
    "                        'form_type': form,\n",
    "                        'primary_document': primary_documents[i] if i < len(primary_documents) else ''\n",
    "                    }\n",
    "                    recent_filings.append(filing_info)\n",
    "            \n",
    "            # Return most recent 5 filings of the specified type\n",
    "            return recent_filings[:5]\n",
    "            \n",
    "        except (KeyError, IndexError) as e:\n",
    "            logger.error(f\"Error extracting filings: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def initialize_csv(self):\n",
    "        \"\"\"Initialize CSV file with headers\"\"\"\n",
    "        with open(self.csv_filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                'session_id',\n",
    "                'company_name',\n",
    "                'cik',\n",
    "                'filing_date',\n",
    "                'form_type',\n",
    "                'accession_number',\n",
    "                'match_type',\n",
    "                'matched_text',\n",
    "                'context',\n",
    "                'relevance_score',\n",
    "                'extraction_timestamp',\n",
    "                'research_timestamp'\n",
    "            ])\n",
    "    \n",
    "    def save_finding_to_csv(self, finding: Dict[str, Any], cik: str, form_type: str, accession_number: str):\n",
    "        \"\"\"Save individual finding to CSV\"\"\"\n",
    "        with open(self.csv_filename, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                self.collector.session_id,\n",
    "                finding['company'],\n",
    "                cik,\n",
    "                finding['filing_date'],\n",
    "                form_type,\n",
    "                accession_number,\n",
    "                finding['match_type'],\n",
    "                finding['matched_text'],\n",
    "                finding['context'],\n",
    "                finding['relevance_score'],\n",
    "                finding['extraction_timestamp'],\n",
    "                datetime.now().isoformat()\n",
    "            ])\n",
    "    \n",
    "    def conduct_research(self):\n",
    "        \"\"\"Main research method to collect and analyze SEC filings\"\"\"\n",
    "        logger.info(f\"Starting SEC cybersecurity research session: {self.collector.session_id}\")\n",
    "        logger.info(f\"Output file: {self.csv_filename}\")\n",
    "        \n",
    "        self.initialize_csv()\n",
    "        total_findings = 0\n",
    "        companies_processed = 0\n",
    "        \n",
    "        for company_name, cik in self.COMPANY_DATABASE.items():\n",
    "            try:\n",
    "                logger.info(f\"Processing {company_name} (CIK: {cik})\")\n",
    "                \n",
    "                # Get company filings\n",
    "                filings_data = self.get_company_filings(cik)\n",
    "                if not filings_data:\n",
    "                    logger.warning(f\"No filings data for {company_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract recent 8-K filings (current reports)\n",
    "                recent_filings = self.extract_recent_filings(filings_data, '8-K')\n",
    "                logger.info(f\"Found {len(recent_filings)} recent 8-K filings for {company_name}\")\n",
    "                \n",
    "                company_findings = 0\n",
    "                \n",
    "                for filing in recent_filings:\n",
    "                    # Get filing content\n",
    "                    content = self.get_filing_content(filing['accession_number'], cik)\n",
    "                    if not content:\n",
    "                        continue\n",
    "                    \n",
    "                    # Process for breach mentions\n",
    "                    findings = self.processor.extract_breach_mentions(\n",
    "                        content, company_name, filing['filing_date']\n",
    "                    )\n",
    "                    \n",
    "                    # Save findings to CSV\n",
    "                    for finding in findings:\n",
    "                        self.save_finding_to_csv(\n",
    "                            finding, cik, filing['form_type'], filing['accession_number']\n",
    "                        )\n",
    "                        company_findings += 1\n",
    "                        total_findings += 1\n",
    "                    \n",
    "                    # Brief pause between filings\n",
    "                    time.sleep(random.uniform(0.5, 1.5))\n",
    "                \n",
    "                logger.info(f\"Completed {company_name}: {company_findings} findings\")\n",
    "                companies_processed += 1\n",
    "                \n",
    "                # Strategic pause between companies\n",
    "                if companies_processed < len(self.COMPANY_DATABASE):\n",
    "                    pause = random.uniform(2, 5)\n",
    "                    logger.info(f\"Pausing for {pause:.1f}s before next company\")\n",
    "                    time.sleep(pause)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {company_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Final summary\n",
    "        stats = self.collector.get_session_stats()\n",
    "        logger.info(f\"Research complete. Processed {companies_processed} companies, found {total_findings} total findings\")\n",
    "        logger.info(f\"Session stats: {stats}\")\n",
    "        logger.info(f\"Data saved to: {self.csv_filename}\")\n",
    "        \n",
    "        return total_findings\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        collector = SECDataCollector()\n",
    "        findings_count = collector.conduct_research()\n",
    "        \n",
    "        print(f\"\\n=== RESEARCH COMPLETE ===\")\n",
    "        print(f\"Total findings: {findings_count}\")\n",
    "        print(f\"Output file: {collector.csv_filename}\")\n",
    "        print(f\"Companies analyzed: {len(collector.COMPANY_DATABASE)}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Research interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Research failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
